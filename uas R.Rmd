---
title: "uas R"
author: "Kelompok 1"
date: "2023-12-19"
output: html_document
---

## IMPORT LIBRARY
```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(tidyverse)
library(psych) #Untuk melihat data normal
library(moments) #Untuk menghitung Pearson Kurtosis, Geary Skewness
library(outliers) #Untuk mendeteksi outliers
library(Metrics)
library(randomForest)
library(e1071)
library(janitor)
library(shiny)
library(class)
library(gbm)
library(neuralnet)
library(pROC)
library(rpart)
library(rpart.plot)
```

## LOADING DATASET
```{r}

admisi = read.csv('admisi.csv', header = TRUE, sep = ";")
admisi
profiling = read.csv('profiling.csv', header = TRUE, sep = ";")
profiling

```



## DATA PREPROCESSING 1: HANDLING MISSING VALUE
```{r}

cek_NA_GRE = sum(is.na(admisi$GRE))
cek_NA_TOEFL = sum(is.na(admisi$TOEFL))
cek_NA_MLETTER = sum(is.na(admisi$MOT_LETTER))
cek_NA_RLETTER = sum(is.na(admisi$REKOM_LETTER))
cek_NA_LULUS = sum(is.na(admisi$LULUS))
cek_NA_USIA = sum(is.na(profiling$USIA))
cek_NA_JENKEL = sum(is.na(profiling$JENIS_KEL))
cek_NA_RUA = sum(is.na(profiling$Raking_Uni_Asal))
cek_NA_IPK = sum(is.na(profiling$IPK))
cek_NA_RISET = sum(is.na(profiling$RISET))

# Menghitung modus
modus_jenkel = names(sort(table(profiling$JENIS_KEL), decreasing = TRUE)[1])
modus_riset = names(sort(table(profiling$RISET), decreasing = TRUE)[1])
modus_lulus = names(sort(table(admisi$LULUS), decreasing = TRUE)[1])

# Mengganti nilai NA dengan modus
profiling$JENIS_KEL[is.na(profiling$JENIS_KEL)] = modus_jenkel
profiling$RISET[is.na(profiling$RISET)] = modus_riset
admisi$LULUS[is.na(admisi$LULUS)] = modus_lulus

# Transformasi faktor
profiling$JENIS_KEL = as.factor(profiling$JENIS_KEL)
admisi$LULUS = factor(admisi$LULUS, levels = c(1, 0), labels = c('Lulus', 'Tidak Lulus'))
profiling$RISET = as.factor(profiling$RISET)

# Mengganti koma (,) dengan titik (.) pada kolom "IPK"
profiling$IPK = gsub(",", ".", profiling$IPK)
admisi$MOT_LETTER = gsub(",", ".", admisi$MOT_LETTER)
admisi$REKOM_LETTER <- gsub(",", ".", admisi$REKOM_LETTER)

# Mengganti nilai NA pada nilai non-numerik
profiling$IPK = as.numeric(profiling$IPK)
admisi$MOT_LETTER = as.numeric(admisi$MOT_LETTER)
admisi$REKOM_LETTER = as.numeric(admisi$REKOM_LETTER)

# Menghitung mean
mean_rank = mean(profiling$Ranking_Uni_Asal, na.rm = TRUE)
mean_motl = mean(admisi$MOT_LETTER, na.rm = TRUE)
mean_koml = mean(admisi$REKOM_LETTER, na.rm = TRUE)

# Mengganti nilai NA dengan mean
profiling$Ranking_Uni_Asal[is.na(profiling$Ranking_Uni_Asal)] = mean_rank
admisi$MOT_LETTER[is.na(admisi$MOT_LETTER)] = mean_motl
admisi$REKOM_LETTER[is.na(admisi$REKOM_LETTER)] = mean_koml

str(admisi)
str(profiling)

```


## DATA PREPROCESSING 2A: HANDLING DUPLICATED DATA ADMISI
```{R}

# Cek Data Duplikat
cek_admisi = get_dupes(admisi)
cek_admisi

# Mempertahankan satu baris dari setiap kelompok duplikat
newdata_admisi = distinct(admisi, .keep_all = TRUE)

```


## DATA PREPROCESSING 2B: HANDLING DUPLICATED DATA PROFILING
```{r}

# Cek Data Duplikat
cek_profiling = get_dupes(profiling)
cek_profiling

# Mempertahankan satu baris dari setiap kelompok duplikat
newdata_profiling = distinct(profiling, .keep_all = TRUE)

```


## DATA PREPROCESSING 3A: HANDLING OUTLIER ADMISI

```{r}

# Loop untuk menampilkan boxplot satu per satu untuk variabel numerik
for (col in names(newdata_admisi)) {
  if (is.numeric(newdata_admisi[[col]])) {
    boxplot(newdata_admisi[[col]], main = col)
  }
}

# Menghapus Outliers
IQR_koml = IQR(newdata_admisi$REKOM_LETTER)
lowfen_koml = quantile(newdata_admisi$REKOM_LETTER, 0.25) - 1.5 * IQR_koml

clean_admisi = subset(newdata_admisi, REKOM_LETTER >= lowfen_koml)

# Mengecek apakah masih terdapat outliers
for (col in names(clean_admisi)) {
  if (is.numeric(clean_admisi[[col]])) {
    boxplot(clean_admisi[[col]], main = col)
  }
}

```



## DATA PREPROCESSING 3B: HANDLING OUTLIER PROFILING

```{r}

# Loop untuk menampilkan boxplot satu per satu untuk variabel numerik
for (col in names(newdata_profiling)) {
  if (is.numeric(newdata_profiling[[col]])) {
    boxplot(newdata_profiling[[col]], main = col)
  }
}

# Menghapus Outliers
IQR_ipk = IQR(newdata_profiling$IPK)
upfen_ipk = quantile(newdata_profiling$IPK, 0.75) + 1.5 * IQR_ipk

clean_profiling = subset(newdata_profiling, IPK <= upfen_ipk)

# Mengecek apakah masih terdapat outliers
for (col in names(clean_profiling)) {
  if (is.numeric(clean_profiling[[col]])) {
    boxplot(clean_profiling[[col]], main = col)
  }
}

```


## MERGE DATASET
```{r}

# Menggabungkan dataset berdasarkan kolom ID
newdata = merge(clean_profiling, clean_admisi, by = "ID")
newdata

```


## FEATURE SELECTION
```{r}

control = trainControl(method="repeatedcv", number=10, repeats=3)
model = train(LULUS ~ ., data = newdata[, setdiff(names(newdata), "NAMA")], method = "lvq", preProcess = "scale", trControl = control)

# Estimasi Feature yang Penting
importance = varImp(model, scale=FALSE)

# plot importance
plot(importance)

```

## SPLIT DATA
```{r}
# Subset data dengan feature yang dipilih
selected_features <- c("IPK", "RISET", "GRE", "TOEFL", "REKOM_LETTER", "MOT_LETTER", "JENIS_KEL", "USIA")
selected_data <- newdata[, c("LULUS", selected_features)]

# Convert categorical variables to factors
selected_data$JENIS_KEL <- as.factor(selected_data$JENIS_KEL)

# Split the data into training and testing sets
set.seed(123)  # for reproducibility
train_indices <- createDataPartition(selected_data$LULUS, p = 0.8, list = FALSE)
train_data <- selected_data[train_indices, ]
test_data <- selected_data[-train_indices, ]

```


## Model 1 (RF)
```{r}
# Model Training
rf_model <- train(LULUS ~ .,  
                         data = train_data, 
                         method = "rf",
                         trControl = trainControl(method = "cv", number = 5))

# Model Prediction on the test set
predictions <- predict(rf_model, newdata = test_data)

# Calculate AUC and plot ROC
roc_curve <- roc(test_data$LULUS, as.numeric(predictions))
auc_value <- auc(roc_curve)

# Evaluate the model
conf_matrix <- confusionMatrix(predictions, test_data$LULUS)
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass[["Pos Pred Value"]]

# Display evaluation metrics
print(paste("Accuracy:", accuracy))
print(paste("Precision:", precision))
print(paste("AUC:", auc_value))

# Plot ROC curve
plot(roc_curve, main = "ROC Curve for Random Forest", col = "blue", lwd = 2)
```

## MODEL 2 (SVM)
```{r}
# Model Training - Support Vector Machine (SVM)
svm_model <- svm(LULUS ~ ., data = train_data, probability = TRUE)

# Model Prediction on the test set
predictions_svm <- predict(svm_model, newdata = test_data, probability = TRUE)

# Extracting probability of positive class for AUC calculation
svm_probabilities <- attr(predictions_svm, "probabilities")[, 2]

# Evaluate the model
conf_matrix_svm <- confusionMatrix(predictions_svm, test_data$LULUS)
accuracy_svm <- conf_matrix_svm$overall["Accuracy"]
precision_svm <- conf_matrix_svm$byClass[["Pos Pred Value"]]

# Calculate AUC and plot ROC
roc_curve_svm <- roc(test_data$LULUS, svm_probabilities)
auc_value_svm <- auc(roc_curve_svm)

# Display evaluation metrics for SVM
print(paste("SVM Accuracy:", accuracy_svm))
print(paste("SVM Precision:", precision_svm))
print(paste("SVM AUC:", auc_value_svm))

# Plot ROC curve
plot(roc_curve_svm, main = "ROC Curve for SVM", col = "blue", lwd = 2)
```

## Model 3 (LR)
```{r}
# Convert "Lulus" to 1 and "Tidak Lulus" to 0
train_data_lr <- train_data %>%
  mutate(LULUS = ifelse(LULUS == "Lulus", 1, 0))

# Model Training - Logistic Regression (LR)
lr_model <- glm(LULUS ~ ., data = train_data_lr, family = "binomial")

# Model Prediction on the test set
predictions_lr <- predict(lr_model, newdata = test_data, type = "response")

# Convert predicted probabilities to class labels
predicted_classes_lr <- ifelse(predictions_lr > 0.5, "Lulus", "Tidak Lulus")

# Evaluate the model
conf_matrix_lr <- confusionMatrix(factor(predicted_classes_lr), test_data$LULUS)
accuracy_lr <- conf_matrix_lr$overall["Accuracy"]
precision_lr <- conf_matrix_lr$byClass[["Pos Pred Value"]]

# Calculate AUC and plot ROC
roc_curve_lr <- roc(test_data$LULUS, predictions_lr)
auc_value_lr <- auc(roc_curve_lr)

# Display evaluation metrics for LR
print(paste("LR Accuracy:", accuracy_lr))
print(paste("LR Precision:", precision_lr))
print(paste("LR AUC:", auc_value_lr))

# Plot ROC curve
plot(roc_curve_lr, main = "ROC Curve for Logistic Regression", col = "blue", lwd = 2)
```

## Model 4 (GBM)
```{r}
# Convert "Lulus" to 1 and "Tidak Lulus" to 0
train_data_gbm <- train_data %>%
  mutate(LULUS = ifelse(LULUS == "Lulus", 1, 0))

# Model Training - Gradient Boosting Machine (GBM)
gbm_model <- gbm(LULUS ~ ., data = train_data_gbm, distribution = "bernoulli", n.trees = 100, interaction.depth = 3)

# Model Prediction on the test set
predictions_gbm <- predict(gbm_model, newdata = test_data, n.trees = 100)

# Convert predicted values to probabilities
probabilities_gbm <- 1 / (1 + exp(-predictions_gbm))

# Convert predicted probabilities to class labels
predicted_classes_gbm <- ifelse(probabilities_gbm > 0.5, "Lulus", "Tidak Lulus")

# Evaluate the model
conf_matrix_gbm <- confusionMatrix(factor(predicted_classes_gbm), test_data$LULUS)
accuracy_gbm <- conf_matrix_gbm$overall["Accuracy"]
precision_gbm <- conf_matrix_gbm$byClass[["Pos Pred Value"]]

# Calculate AUC and plot ROC
roc_curve_gbm <- roc(test_data$LULUS, probabilities_gbm)
auc_value_gbm <- auc(roc_curve_gbm)

# Display evaluation metrics for GBM
print(paste("GBM Accuracy:", accuracy_gbm))
print(paste("GBM Precision:", precision_gbm))
print(paste("GBM AUC:", auc_value_gbm))

# Plot ROC curve
plot(roc_curve_gbm, main = "ROC Curve for Gradient Boosting Machine", col = "blue", lwd = 2)
```

## MODEL 5 (DT)
```{r}
# Convert "Lulus" to 1 and "Tidak Lulus" to 0
train_data_dt <- train_data %>%
  mutate(LULUS = ifelse(LULUS == "Lulus", 1, 0))

# Model Training - Decision Tree (DT)
dt_model <- rpart(LULUS ~ ., data = train_data_dt, method = "class")

# Model Prediction on the test set
predictions_dt <- predict(dt_model, newdata = test_data, type = "class")

# Convert predicted class to labels
predicted_classes_dt <- ifelse(predictions_dt == 1, "Lulus", "Tidak Lulus")

# Evaluate the model
conf_matrix_dt <- confusionMatrix(factor(predicted_classes_dt), test_data$LULUS)
accuracy_dt <- conf_matrix_dt$overall["Accuracy"]
precision_dt <- conf_matrix_dt$byClass[["Pos Pred Value"]]

# Display evaluation metrics for Decision Tree
print(paste("Decision Tree Accuracy:", accuracy_dt))
print(paste("Decision Tree Precision:", precision_dt))

# Create ROC curve
roc_curve_dt <- roc(test_data$LULUS, as.numeric(predictions_dt))
auc_value_dt <- auc(roc_curve_dt)

# Display AUC for Decision Tree
print(paste("Decision Tree AUC:", auc_value_dt))

# Plot ROC curve
plot(roc_curve_dt, main = "ROC Curve for Decision Tree", col = "blue", lwd = 2)
```


## MODEL 6 (KNN)
```{r}

# Preprocess the data
preProcValues <- preProcess(train_data, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, train_data)
testTransformed <- predict(preProcValues, test_data)

# Train the model
knnModel <- train(
		     LULUS ~ ., 
		     data = trainTransformed, 
		     method = "knn", 
		     trControl = trainControl(method = "cv"), 
		     tuneGrid = expand.grid(k=1:100))

# Train K-Nearest Neighbors with the best k
best_model<- knn3(
                  LULUS ~ .,
                  data = trainTransformed,
                  k = knnModel$bestTune$k
                 )

# Make predictions on the test set
predictions <- predict(best_model, testTransformed,type = "class")
probabilities_knn <- as.numeric(predictions_knn[, "Lulus"])

# Calculate confusion matrix
conf_matrix_knn <- confusionMatrix(predictions, testTransformed$LULUS)
accuracy_knn <- conf_matrix_knn$overall["Accuracy"]
precision_knn <- conf_matrix_knn$byClass[["Pos Pred Value"]]

# Display evaluation metrics for Decision Tree
print(paste("K-Nearest Neighbors Accuracy:", accuracy_knn))
print(paste("K-Nearest Neighbors Precision:", precision_knn))

# Display evaluation metrics for k-NN
print(paste("K-Nearest Neighbors Accuracy:", accuracy_knn))
print(paste("K-Nearest Neighbors Precision:", precision_knn))

# Create ROC curve
roc_curve_knn <- roc(testTransformed$LULUS, probabilities_knn)
auc_value_knn <- auc(roc_curve_knn)

# Display AUC for k-NN
print(paste("K-Nearest Neighbors AUC:", auc_value_knn))

# Plot ROC curve for k-NN
plot(roc_curve_knn, main = "ROC Curve for k-NN", col = "blue", lwd = 2)


```
## HASIL NARASI

#Presisi dalam konteks ini merujuk pada proporsi dari prediksi positif yang benar (True Positive) dibandingkan dengan total prediksi positif (True Positive + False Positive). Dalam kata lain, presisi mengukur sejauh mana prediksi positif yang diberikan oleh model benar atau relevan. Dalam kasus ini, IEDU ingin memastikan bahwa ketika alat prediktif memberikan saran untuk melanjutkan aplikasi MBA di UoU, prediksi tersebut memiliki tingkat ketepatan yang tinggi, minimal 85%.

# Confusion matrix adalah alat evaluasi kinerja model yang menyajikan hasil prediksi dalam empat kategori: True Positive (TP), False Positive (FP), True Negative (TN), dan False Negative (FN).
#True Positive (TP):
#Definisi: Jumlah observasi yang sebenarnya positif dan diprediksi positif dengan benar.
#Konteks: Jumlah calon mahasiswa yang benar-benar berhasil di UoU dan diprediksi dengan benar oleh model.

#False Positive (FP):
#Definisi: Jumlah observasi yang sebenarnya negatif tetapi diprediksi positif.
#Konteks: Jumlah calon mahasiswa yang sebenarnya tidak berhasil di UoU, tetapi model memprediksi bahwa mereka berhasil.

#True Negative (TN):
#Definisi: Jumlah observasi yang sebenarnya negatif dan diprediksi negatif dengan benar.
#Konteks: Jumlah calon mahasiswa yang tidak berhasil di UoU dan model dengan benar memprediksi bahwa mereka tidak berhasil.

#False Negative (FN):
#Definisi: Jumlah observasi yang sebenarnya positif tetapi diprediksi negatif.
#Konteks: Jumlah calon mahasiswa yang sebenarnya berhasil di UoU, tetapi model gagal memprediksi keberhasilan mereka.

#Presisi memberikan gambaran seberapa akurat model dalam mengidentifikasi calon mahasiswa yang sebenarnya berhasil di UoU dari total calon yang diprediksi berhasil. Dalam konteks ini, IEDU menetapkan target presisi sebesar 85%, yang berarti ingin meminimalkan jumlah prediksi positif yang salah

#Features yang diambil yaitu IPK, RISET, GRE, TOEFL, REKOM_LETTER, MOT_LETTER, JENIS_KEL, USIA

# IPK : bisa menjadi indikator kuat untuk kinerja akademis calon mahasiswa. IPK cenderung berkorelasi dengan keberhasilan studi lanjutan.
# RISET : RISET atau pengalaman penelitian dapat mencerminkan dedikasi dan keterampilan penelitian calon mahasiswa. Terutama jika program MBA di UoU memiliki fokus pada aspek riset.
# GRE dan TOEFL : Skor GRE dan TOEFL bisa menjadi standart untuk melihat kemampuan akademis dan kemahiran berbahasa inggris calon mahasiswa. Keduanya dianggap penting dalam penilaian dan kelayakan penerimaan.
# REKOM_LETTER dan MOT_LETTER : Skor surat rekomendasi dan motivation letter memberikan wawasan tentang kemampuan interpersonal dan motivasi calon mahasiswa. 
# JENIS_KEL dan USIA : Jenis kelamin dan usia memberikan informasi tambahan tentang karakteristik pendaftar. Meskipun label ini tergantung dengan kebijakan masing-masing universitas. Tetapi tetap features ini membantu dalam memberikan korelasi dan peran dalam nilai yang dihasilkan dalam akurasi, presisi, dan AUC pada model.

#Penggunaan SVM pada model

# #shiny sementara
```{r}
ui = fluidPage(
  titlePanel("Judul"),
  sidebarLayout(
    sidebarPanel(
      selectInput('JENIS_KEL', 'Pilih Jenis Kelamin Anda', choices = c('Perempuan', 'Laki-Laki'), 
                  selected = 'Perempuan'),
      sliderInput('USIA', 'Pilih Usia Anda', 
                  min = 1,
                  max = 50,
                  value = 0),
      numericInput('IPK','Masukan IPK Anda : ', value=0),
      numericInput('TOEFL','Masukan Skor TOEFL Anda : ', value=0),
      numericInput('GRE','Masukan Skor Gre Anda : ', value=0),
      numericInput('REKOM_LETTER','Masukan Skor REKOM_LETTER Anda : ', value=0),
      numericInput('MOT_LETTER','Masukan Skor MOT_LETTER Anda : ', value=0),
      selectInput('RISET', 'Apakah Anda Melakukan Riset Sebelumnya', choices = c('Ya', 'Tidak'), 
                  selected = 'Ya'),
      actionButton('hitung', 'Prediksi Kelulusan')
    ), 
    mainPanel(
      textOutput('hasil')
    )
  )
)

server = function(input, output){
  observeEvent(input$hitung, {
    predictions <- predict(model, newdata=data.frame(JENIS_KEL=input$JENIS_KEL, USIA=input$USIA, 
                                                     IPK=input$IPK, TOEFL=input$TOEFL, GRE=input$GRE, 
                                                     REKOM_LETTER=input$REKOM_LETTER, MOT_LETTER=input$MOT_LETTER,
                                                     RISET=input$RISET))
    output$hasil <- renderText({paste('Hasil Prediksi Kelulusan adalah', predictions)})
    })
}

shinyApp(ui=ui, server=server)
```